# Oracle to PostgreSQL Migration Tool Configuration Template
#
# This is the main configuration file for the Oracle to PostgreSQL migration tool.
# Copy this file to config.yaml and update the values according to your environment.
#
# The tool consists of three main scripts:
# 1. analyze_sql.py - Analyzes Oracle SQL files and generates PostgreSQL DDL
# 2. create_tables.py - Creates tables in PostgreSQL using generated DDL
# 3. import_data.py - Imports data from Oracle SQL files to PostgreSQL
#
# All scripts can use this configuration file with the --config parameter.

# =============================================================================
# BASIC CONFIGURATION
# =============================================================================

# Directory containing Oracle SQL dump files (required)
# This should point to the directory where your .sql files are located
source_directory: "/sas_1/backup/xindu-backup-250815/"

# Directory to store generated DDL files (optional)
# The analyze_sql.py script will create DDL files in this directory
ddl_directory: "./ddl"

# Number of lines to sample from each SQL file for analysis (optional)
# Higher values provide better analysis but slower processing
# Recommended: 100-500 for most cases
sample_lines: 100

# Maximum number of INSERT statements to send to DeepSeek for DDL generation (optional)
# Higher values provide better table structure analysis but cost more API tokens
# Recommended: 10-30 for most cases, 50+ for complex tables
max_insert_samples: 20

# Target encoding for PostgreSQL (optional)
# Most PostgreSQL installations use UTF-8
target_encoding: "utf-8"

# =============================================================================
# DEEPSEEK API CONFIGURATION
# =============================================================================
# Required for analyze_sql.py script to generate DDL statements

deepseek:
  # Your DeepSeek API key (required)
  # Get your API key from: https://platform.deepseek.com/
  api_key: "sk-fe8ac294afa645949441fe502754f2b2"

  # DeepSeek API base URL (optional)
  # Usually no need to change this
  base_url: "https://api.deepseek.com"

  model: "deepseek-reasoner"

  # API request timeout in seconds (optional)
  # Increase if you have slow network connection
  timeout: 120

  # Maximum number of retry attempts for failed API calls (optional)
  # Helps handle temporary network issues
  max_retries: 3
  
  # Auto fallback to deepseek-chat if deepseek-reasoner fails (optional)
  # Helps ensure DDL generation succeeds even if reasoner model has issues
  auto_fallback: true

# =============================================================================
# POSTGRESQL DATABASE CONFIGURATION
# =============================================================================
# Required for create_tables.py and import_data.py scripts

postgresql:
  # PostgreSQL server hostname or IP address
  host: "localhost"

  # PostgreSQL server port (default: 5432)
  port: 5432

  # Target database name (required)
  # This database must already exist
  database: "xindu"

  # Target schema name (optional)
  # Tables will be created in this schema
  schema: "public"

  # PostgreSQL username (required)
  # User must have CREATE TABLE and INSERT privileges
  username: "postgres"

  # PostgreSQL password (required)
  # Consider using environment variables for security
  password: "ssPg!23"

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================
# Tuning parameters for large file processing

performance:
  # Maximum number of parallel workers for data import (optional)
  # Recommended: 2-8 depending on your system and database capacity
  # Higher values may overwhelm the database
  max_workers: 16

  # Batch size for processing INSERT statements (optional)
  # Larger batches are more efficient but use more memory
  # Recommended: 500-2000 for most cases
  batch_size: 10000

  # Memory limit in MB for file processing (optional)
  # Helps prevent out-of-memory errors with large files
  memory_limit_mb: 1024

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Controls logging output and verbosity

logging:
  # Log level: DEBUG, INFO, WARNING, ERROR (optional)
  # DEBUG: Very detailed output (use for troubleshooting)
  # INFO: Normal operation information (recommended)
  # WARNING: Only warnings and errors
  # ERROR: Only errors
  level: "INFO"

  # Log file path (optional)
  # All log messages will be written to this file
  # Use absolute path or relative to script location
  file: "./migration.log"

# =============================================================================
# EXAMPLE CONFIGURATIONS FOR DIFFERENT SCENARIOS
# =============================================================================

# Example 1: Small dataset (< 1GB total)
# performance:
#   max_workers: 2
#   batch_size: 500
#   memory_limit_mb: 512

# Example 2: Large dataset (> 10GB total)
# performance:
#   max_workers: 8
#   batch_size: 2000
#   memory_limit_mb: 2048

# Example 3: Development/testing environment
# logging:
#   level: "DEBUG"
#   file: "./debug.log"
# performance:
#   max_workers: 1
#   batch_size: 100