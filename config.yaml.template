# Oracle to PostgreSQL Migration Tool Configuration Template
# 
# This is the main configuration file for the Oracle to PostgreSQL migration tool.
# Copy this file to config.yaml and update the values according to your environment.
#
# The tool consists of three main scripts:
# 1. analyze_sql.py - Analyzes Oracle SQL files and generates PostgreSQL DDL
# 2. create_tables.py - Creates tables in PostgreSQL using generated DDL
# 3. import_data.py - Imports data from Oracle SQL files to PostgreSQL
#
# All scripts can use this configuration file with the --config parameter.

# =============================================================================
# BASIC CONFIGURATION
# =============================================================================

# Directory containing Oracle SQL dump files (required)
# This should point to the directory where your .sql files are located
source_directory: "/path/to/oracle/dumps"

# Directory to store generated DDL files (optional)
# The analyze_sql.py script will create DDL files in this directory
ddl_directory: "./ddl"

# Number of lines to sample from each SQL file for analysis (optional)
# Higher values provide better analysis but slower processing for large files
# Recommended values:
#   - 100-500: Fast processing, good for most cases
#   - 1000-2000: Better analysis for complex schemas
#   - 5000+: Comprehensive analysis (slower for large files)
sample_lines: 500

# Target encoding for PostgreSQL (optional)
# Most PostgreSQL installations use UTF-8
target_encoding: "utf-8"

# =============================================================================
# DEEPSEEK API CONFIGURATION
# =============================================================================
# Required for analyze_sql.py script to generate DDL statements

deepseek:
  # Your DeepSeek API key (required)
  # Get your API key from: https://platform.deepseek.com/
  api_key: "your-deepseek-api-key-here"
  
  # DeepSeek model to use (optional)
  # Available models:
  #   - deepseek-reasoner: Best for complex reasoning tasks (default, recommended)
  #   - deepseek-chat: General purpose chat model
  #   - deepseek-coder: Optimized for code generation
  model: "deepseek-reasoner"
  
  # DeepSeek API base URL (optional)
  # Usually no need to change this
  base_url: "https://api.deepseek.com"
  
  # API request timeout in seconds (optional)
  # Increase if you have slow network connection
  timeout: 30
  
  # Maximum number of retry attempts for failed API calls (optional)
  # Helps handle temporary network issues
  max_retries: 3

# =============================================================================
# POSTGRESQL DATABASE CONFIGURATION
# =============================================================================
# Required for create_tables.py and import_data.py scripts

postgresql:
  # PostgreSQL server hostname or IP address
  host: "localhost"
  
  # PostgreSQL server port (default: 5432)
  port: 5432
  
  # Target database name (required)
  # This database must already exist
  database: "your_target_database"
  
  # Target schema name (optional)
  # Tables will be created in this schema
  schema: "public"
  
  # PostgreSQL username (required)
  # User must have CREATE TABLE and INSERT privileges
  username: "your_username"
  
  # PostgreSQL password (required)
  # Consider using environment variables for security
  password: "your_password"

# =============================================================================
# PERFORMANCE CONFIGURATION
# =============================================================================
# Tuning parameters for large file processing

performance:
  # Maximum number of parallel workers for data import (optional)
  # Recommended: 2-8 depending on your system and database capacity
  # Higher values may overwhelm the database
  max_workers: 4
  
  # Batch size for processing INSERT statements (optional)
  # Larger batches are more efficient but use more memory
  # Recommended: 500-2000 for most cases
  batch_size: 1000
  
  # Memory limit in MB for file processing (optional)
  # Helps prevent out-of-memory errors with large files
  memory_limit_mb: 1024

# =============================================================================
# LOGGING CONFIGURATION
# =============================================================================
# Controls logging output and verbosity

logging:
  # Log level: DEBUG, INFO, WARNING, ERROR (optional)
  # DEBUG: Very detailed output (use for troubleshooting)
  # INFO: Normal operation information (recommended)
  # WARNING: Only warnings and errors
  # ERROR: Only errors
  level: "INFO"
  
  # Log file path (optional)
  # All log messages will be written to this file
  # Use absolute path or relative to script location
  file: "./migration.log"
  
  # Show detailed progress steps (optional)
  # true: Shows each step (encoding detection, parsing, DeepSeek API, saving)
  # false: Shows simple file-by-file progress
  show_progress_steps: true

# =============================================================================
# EXAMPLE CONFIGURATIONS FOR DIFFERENT SCENARIOS
# =============================================================================

# Example 1: Small dataset (< 1GB total)
# performance:
#   max_workers: 2
#   batch_size: 500
#   memory_limit_mb: 512

# Example 2: Large dataset (> 10GB total)
# performance:
#   max_workers: 8
#   batch_size: 2000
#   memory_limit_mb: 2048

# Example 3: Development/testing environment
# logging:
#   level: "DEBUG"
#   file: "./debug.log"
# performance:
#   max_workers: 1
#   batch_size: 100