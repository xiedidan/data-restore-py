# Oracle to PostgreSQL Migration Tool - Usage Guide

This guide provides detailed instructions for using the Oracle to PostgreSQL migration tool. The tool consists of three main scripts that work together to migrate Oracle SQL dump files to PostgreSQL.

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Configuration](#configuration)
3. [Script 1: analyze_sql.py](#script-1-analyze_sqlpy)
4. [Script 2: create_tables.py](#script-2-create_tablespy)
5. [Script 3: import_data.py](#script-3-import_datapy)
6. [Complete Migration Workflow](#complete-migration-workflow)
7. [Command Line Reference](#command-line-reference)
8. [Output Files](#output-files)

## Prerequisites

Before using the migration tool, ensure you have:

1. **Python 3.8 or higher** installed
2. **Required Python packages** installed:
   ```bash
   pip install -r requirements.txt
   ```
3. **DeepSeek API key** for DDL generation
4. **PostgreSQL database** with appropriate permissions
5. **Oracle SQL dump files** in a directory

## Configuration

### Step 1: Create Configuration File

Copy the template configuration file and customize it:

```bash
cp config.yaml.template config.yaml
```

Edit `config.yaml` with your specific settings:

```yaml
# Minimum required configuration
source_directory: "/path/to/your/oracle/dumps"
deepseek:
  api_key: "your-actual-api-key"
  model: "deepseek-reasoner"  # Optional: deepseek-reasoner (default), deepseek-chat, or deepseek-coder
postgresql:
  database: "your_target_database"
  username: "your_username"
  password: "your_password"
```

### Step 2: Verify Configuration

Test your configuration with a dry run:

```bash
python analyze_sql.py --config config.yaml --help
```

## Script 1: analyze_sql.py

**Purpose**: Analyzes Oracle SQL dump files, detects encoding, and generates PostgreSQL DDL statements.

### Basic Usage

```bash
# Using configuration file (recommended)
python analyze_sql.py --config config.yaml

# Using command line arguments (overrides config file)
python analyze_sql.py \
  --source-directory /path/to/dumps \
  --deepseek-api-key your-api-key \
  --deepseek-model deepseek-reasoner \
  --sample-lines 200

# Mixed approach (config file + command line overrides)
python analyze_sql.py --config config.yaml --deepseek-model deepseek-coder
```

### What It Does

1. **Scans** the source directory for `.sql` files
2. **Detects** file encoding (UTF-8, GBK, etc.)
3. **Parses** INSERT statements to understand table structure
4. **Generates** PostgreSQL DDL using DeepSeek API
5. **Saves** DDL files to the `ddl/` directory
6. **Creates** an analysis report in CSV format

### Output Files

- `ddl/create_tablename.sql` - Generated DDL for each table
- `reports/analysis_report_YYYYMMDD_HHMMSS.csv` - Analysis summary

### Example Output

```
Starting SQL file analysis...
Found 15 SQL files in /data/oracle_dumps/
Processing file1.sql... [Encoding: UTF-8] [Table: users] ✓
Processing file2.sql... [Encoding: GBK] [Table: orders] ✓
Processing file3.sql... [Encoding: UTF-8] [Table: products] ✓
...
Analysis complete! Generated 15 DDL files.
Report saved to: reports/analysis_report_20241215_143022.csv
```

### Common Options

| Option | Description | Example |
|--------|-------------|---------|
| `--source-directory` | Directory with SQL files | `/data/dumps` |
| `--sample-lines` | Lines to analyze per file | `500` |
| `--deepseek-api-key` | API key for DDL generation | `sk-xxx` |
| `--log-level` | Logging verbosity | `DEBUG` |

## Script 2: create_tables.py

**Purpose**: Creates tables in PostgreSQL using the DDL files generated by `analyze_sql.py`.

### Basic Usage

```bash
# Using configuration file
python create_tables.py --config config.yaml

# Using command line arguments (overrides config file)
python create_tables.py \
  --pg-database mydb \
  --pg-username postgres \
  --pg-password mypass \
  --drop-existing
```

### What It Does

1. **Reads** DDL files from the `ddl/` directory
2. **Connects** to PostgreSQL database
3. **Optionally drops** existing tables (if `--drop-existing` is used)
4. **Creates** tables using the generated DDL
5. **Reports** success/failure for each table

### Output

```
Connecting to PostgreSQL database...
Connection successful: postgresql://postgres@localhost:5432/mydb
Found 15 DDL files to process.

Creating table: users... ✓
Creating table: orders... ✓
Creating table: products... ✓
Creating table: order_items... ✗ (Error: column type not supported)
...
Table creation complete! 14/15 tables created successfully.
Report saved to: reports/table_creation_report_20241215_143525.csv
```

### Common Options

| Option | Description | Example |
|--------|-------------|---------|
| `--pg-database` | Target database name | `mydb` |
| `--pg-host` | PostgreSQL host | `localhost` |
| `--pg-port` | PostgreSQL port | `5432` |
| `--pg-schema` | Target schema | `public` |
| `--drop-existing` | Drop tables before creating | (flag) |

## Script 3: import_data.py

**Purpose**: Imports data from Oracle SQL files to PostgreSQL with parallel processing.

### Basic Usage

```bash
# Using configuration file
python import_data.py --config config.yaml

# Using command line arguments (requires analysis report)
python import_data.py \
  --source-directory /path/to/dumps \
  --pg-database mydb \
  --pg-username postgres \
  --max-workers 4
```

### What It Does

1. **Reads** the analysis report to get encoding information
2. **Processes** SQL files with correct encoding
3. **Rewrites** INSERT statements for PostgreSQL compatibility
4. **Imports** data using parallel workers
5. **Monitors** progress and handles errors
6. **Generates** import statistics report

### Output

```
Starting data import...
Loading encoding information from analysis report...
Found 15 files to import.

Processing files with 4 workers:
[████████████████████████████████████████] 100% (15/15 files)

Import Summary:
- Total files processed: 15
- Successful imports: 14
- Failed imports: 1
- Total records imported: 1,234,567
- Total processing time: 45m 23s
- Average speed: 456 records/second

Report saved to: reports/import_report_20241215_150245.csv
```

### Common Options

| Option | Description | Example |
|--------|-------------|---------|
| `--max-workers` | Parallel import threads | `8` |
| `--batch-size` | Records per batch | `2000` |
| `--source-db-name` | Original Oracle DB name | `ORCL` |
| `--target-db-name` | Target PostgreSQL DB | `mydb` |

## Complete Migration Workflow

Here's the recommended step-by-step process:

### Step 1: Prepare Environment

```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Create configuration
cp config.yaml.template config.yaml
# Edit config.yaml with your settings

# 3. Create necessary directories
mkdir -p ddl reports
```

### Step 2: Analyze SQL Files

```bash
# Analyze all SQL files and generate DDL
python analyze_sql.py --config config.yaml

# Review the analysis report
cat reports/analysis_report_*.csv
```

### Step 3: Create Tables

```bash
# Create tables in PostgreSQL
python create_tables.py --config config.yaml --drop-existing

# Verify tables were created
psql -d mydb -c "\dt"
```

### Step 4: Import Data

```bash
# Import data with parallel processing
python import_data.py --config config.yaml

# Check import results
cat reports/import_report_*.csv
```

### Step 5: Verify Migration

```bash
# Check record counts
psql -d mydb -c "SELECT schemaname, tablename, n_tup_ins FROM pg_stat_user_tables;"
```

## Command Line Reference

### Global Options (All Scripts)

| Option | Short | Description | Default |
|--------|-------|-------------|---------|
| `--config` | `-c` | Configuration file path | `config.yaml` |
| `--log-level` | | Logging level | `INFO` |
| `--log-file` | | Log file path | `./migration.log` |
| `--help` | `-h` | Show help message | |

### analyze_sql.py Specific Options

| Option | Description | Required | Default |
|--------|-------------|----------|---------|
| `--source-directory` | Oracle SQL files directory | Yes | |
| `--sample-lines` | Lines to sample per file | No | `100` |
| `--deepseek-api-key` | DeepSeek API key | Yes | |
| `--deepseek-base-url` | DeepSeek API URL | No | `https://api.deepseek.com` |
| `--deepseek-model` | DeepSeek model to use | No | `deepseek-reasoner` |

### create_tables.py Specific Options

| Option | Description | Required | Default |
|--------|-------------|----------|---------|
| `--pg-database` | PostgreSQL database | Yes | |
| `--pg-host` | PostgreSQL host | No | `localhost` |
| `--pg-port` | PostgreSQL port | No | `5432` |
| `--pg-schema` | PostgreSQL schema | No | `public` |
| `--pg-username` | PostgreSQL username | Yes | |
| `--pg-password` | PostgreSQL password | Yes | |
| `--drop-existing` | Drop existing tables | No | `False` |

### import_data.py Specific Options

| Option | Description | Required | Default |
|--------|-------------|----------|---------|
| `--max-workers` | Parallel workers | No | `4` |
| `--batch-size` | Records per batch | No | `1000` |
| `--source-db-name` | Original Oracle DB name | No | Auto-detect |
| `--target-db-name` | Target PostgreSQL DB | No | From config |

## Output Files

### Analysis Report (CSV)

Generated by `analyze_sql.py`:

```csv
file_name,table_name,encoding,file_size_mb,ddl_generated,error_message
users.sql,users,utf-8,45.2,true,
orders.sql,orders,gbk,123.8,true,
products.sql,products,utf-8,67.1,false,API timeout error
```

### Table Creation Report (CSV)

Generated by `create_tables.py`:

```csv
table_name,ddl_file,success,error_message,execution_time_seconds
users,create_users.sql,true,,2.34
orders,create_orders.sql,true,,1.87
products,create_products.sql,false,column type not supported,0.12
```

### Import Report (CSV)

Generated by `import_data.py`:

```csv
file_name,table_name,records_processed,success,error_message,processing_time_seconds
users.sql,users,50000,true,,45.67
orders.sql,orders,125000,true,,89.23
products.sql,products,0,false,table does not exist,0.05
```

## Tips for Success

1. **Start Small**: Test with a few small files first
2. **Monitor Resources**: Watch CPU, memory, and database connections
3. **Check Logs**: Use `DEBUG` level for troubleshooting
4. **Backup First**: Always backup your PostgreSQL database before importing
5. **Validate Results**: Compare record counts between Oracle and PostgreSQL